{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will classify stimulus classes using the Haxby et al. data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.nitrc.org/frs/download.php/7868/mask.nii.gz ...\n",
      "using data from /Users/poldrack/nilearn_data/haxby2001/subj1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Downloaded 2969 of 2969 bytes (100.0%,    0.0s remaining) ...done. (4 seconds, 0 min)\n"
     ]
    }
   ],
   "source": [
    "import nipype.algorithms.modelgen as model   # model generation\n",
    "import nipype.interfaces.fsl as fsl          # fsl\n",
    "from nipype.interfaces.base import Bunch\n",
    "import os,json,glob\n",
    "import numpy\n",
    "import nibabel\n",
    "import nilearn.plotting\n",
    "import sklearn.multiclass\n",
    "from sklearn.svm import SVC\n",
    "import sklearn.metrics\n",
    "import sklearn.cross_validation\n",
    "from nilearn.input_data import NiftiMasker\n",
    "import scipy.stats\n",
    "import random\n",
    "import nilearn.datasets\n",
    "from haxby_data import HaxbyData\n",
    "\n",
    "haxby_dataset = nilearn.datasets.fetch_haxby()\n",
    "boldfile=haxby_dataset.func[0]\n",
    "datadir=os.path.dirname(boldfile)\n",
    "\n",
    "print('using data from %s'%datadir)\n",
    "haxbydata=HaxbyData(datadir)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "boldimg=nibabel.load(boldfile)\n",
    "\n",
    "if not os.path.exists(boldfile.replace('.nii.gz','_brain.nii.gz')):\n",
    "    bet=fsl.BET()\n",
    "    bet.inputs.in_file=boldfile\n",
    "    bet.inputs.out_file=boldfile.replace('.nii.gz','_brain.nii.gz')\n",
    "    bet.inputs.functional=True\n",
    "    bet.inputs.mask=True\n",
    "    bet.run()\n",
    "\n",
    "\n",
    "brainmaskimg=nibabel.load(boldfile.replace('.nii.gz','_brain_mask.nii.gz'))\n",
    "vtmaskimg=nibabel.load(haxby_dataset.mask_vt[0])\n",
    "\n",
    "# set up design info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modeldir=os.path.join(datadir,'blockmodel')\n",
    "# no way to specify the output directory, so we just chdir into the \n",
    "# desired output directory\n",
    "if not os.path.exists(modeldir):\n",
    "    os.mkdir(modeldir)\n",
    "os.chdir(modeldir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the model with a separate condition for each block using FSL.  This will take several hours to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not os.path.exists(os.path.join(modeldir,'stats/zstat1.nii.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print' (<ipython-input-4-afe4b1de94d7>, line 49)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-afe4b1de94d7>\"\u001b[0;36m, line \u001b[0;32m49\u001b[0m\n\u001b[0;31m    print 'stats have already been run - using existing files'\u001b[0m\n\u001b[0m                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'\n"
     ]
    }
   ],
   "source": [
    "contrasts=[]\n",
    "\n",
    "for i in range(len(haxbydata.conditions)):\n",
    "    contrasts.append([haxbydata.conditions[i],'T',[haxbydata.conditions[i]],[1]])\n",
    "\n",
    "\n",
    "# this is how one could do it using FSL - this is VERY slow, so let's compute the GLM on our own\n",
    "if not os.path.exists(os.path.join(modeldir,'stats/zstat1.nii.gz')):\n",
    "    \n",
    "    \n",
    "    info = [Bunch(conditions=haxbydata.conditions,\n",
    "                  onsets=haxbydata.onsets,\n",
    "                  durations=haxbydata.durations)\n",
    "           ]\n",
    "    s = model.SpecifyModel()\n",
    "    s.inputs.input_units = 'secs'\n",
    "    s.inputs.functional_runs = [haxbydata.boldbrainfile]\n",
    "    s.inputs.time_repetition = haxbydata.tr\n",
    "    s.inputs.high_pass_filter_cutoff = 128.\n",
    "    s.inputs.subject_info = info\n",
    "    s.run()\n",
    "\n",
    "    level1design = fsl.model.Level1Design()\n",
    "    level1design.inputs.interscan_interval = haxbydata.tr\n",
    "    level1design.inputs.bases = {'dgamma':{'derivs': False}}\n",
    "    level1design.inputs.session_info = s._sessinfo\n",
    "    level1design.inputs.model_serial_correlations=False\n",
    "    level1design.inputs.contrasts=contrasts\n",
    "    level1info=level1design.run() \n",
    "    \n",
    "    fsf_file=os.path.join(modeldir,'run0.fsf')\n",
    "    matfile=fsf_file.replace(\".fsf\",\".mat\")\n",
    "    event_files=glob.glob(os.path.join(modeldir,'ev*txt'))\n",
    "\n",
    "    modelgen=fsl.model.FEATModel()\n",
    "    modelgen.inputs.fsf_file=fsf_file\n",
    "    modelgen.inputs.ev_files=event_files\n",
    "    modelgen.run()\n",
    "\n",
    "    fgls = fsl.FILMGLS(autocorr_noestimate=True)\n",
    "    fgls.inputs.in_file =haxbydata.boldbrainfile\n",
    "    fgls.inputs.design_file = os.path.join(modeldir,'run0.mat')\n",
    "    fgls.inputs.threshold = 10\n",
    "    fgls.inputs.results_dir = os.path.join(modeldir,'stats')\n",
    "    fgls.inputs.tcon_file=os.path.join(modeldir,'run0.con')\n",
    "    res = fgls.run() \n",
    "\n",
    "else:\n",
    "    print 'stats have already been run - using existing files'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the zstat images that we will use as our block-by-block signal estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "use_whole_brain=False\n",
    "\n",
    "# include faces and cats\n",
    "condition_mask = numpy.logical_or(haxbydata.condnums == 2,\n",
    "                               haxbydata.condnums == 3)\n",
    "condlabels=haxbydata.condnums[condition_mask]\n",
    "runlabels=haxbydata.runs[condition_mask]\n",
    "\n",
    "if not os.path.exists(os.path.join(modeldir,'zstatdata_facecat.nii.gz')):\n",
    "    zstatdata=numpy.zeros((boldimg.shape[0],boldimg.shape[1],boldimg.shape[2],len(haxbydata.conditions)))\n",
    "    for i in range(len(haxbydata.conditions)):\n",
    "        zstatdata[:,:,:,i]=nibabel.load(os.path.join(modeldir,'stats/zstat%d.nii.gz'%int(i+1))).get_data()\n",
    "\n",
    "    zstatimg=nibabel.Nifti1Image(zstatdata,affine=brainmaskimg.get_affine())\n",
    "    zstatimg.to_filename(os.path.join(modeldir,'zstatdata.nii.gz'))\n",
    "    zstatimg=nibabel.Nifti1Image(zstatdata[:,:,:,condition_mask],affine=brainmaskimg.get_affine())\n",
    "    zstatimg.to_filename(os.path.join(modeldir,'zstatdata_facecat.nii.gz'))\n",
    "\n",
    "if use_whole_brain:\n",
    "    maskimg=haxbydata.brainmaskfile\n",
    "else:\n",
    "    maskimg=haxbydata.vtmaskfile\n",
    "    \n",
    "nifti_masker = NiftiMasker(mask_img=maskimg, standardize=False)\n",
    "fmri_masked = nifti_masker.fit_transform(os.path.join(modeldir,'zstatdata.nii.gz'))\n",
    "fmri_masked = fmri_masked[condition_mask,:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do a leave-one-run out classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shuffle_within_runs(labels,runs):\n",
    "    for r in numpy.unique(runs):\n",
    "        l=labels[runs==r]\n",
    "        random.shuffle(l)\n",
    "        labels[runs==r]=l\n",
    "    return labels\n",
    "\n",
    "\n",
    "def run_classifier(fmri_masked,condlabels,runs,baseclf,shuffle_labels=False):\n",
    "    cv = sklearn.cross_validation.LeaveOneLabelOut(labels=runs)\n",
    "\n",
    "    pred=numpy.zeros(len(runs)) # predicted class\n",
    "\n",
    "    if len(numpy.unique(condlabels))>2:\n",
    "        clf=sklearn.multiclass.OneVsRestClassifier(baseclf)\n",
    "    else:\n",
    "        clf=baseclf\n",
    "    \n",
    "    for train,test in cv:\n",
    "        testdata=fmri_masked[test,:]\n",
    "        traindata=fmri_masked[train,:]\n",
    "        trainlabels=condlabels[train]\n",
    "        if shuffle_labels:\n",
    "            trainlabels=shuffle_within_runs(trainlabels,runs[train])\n",
    "        clf.fit(traindata,trainlabels)\n",
    "        pred[test]=clf.predict(testdata)\n",
    "        \n",
    "    confmtx=sklearn.metrics.confusion_matrix(condlabels,pred)\n",
    "    acc=sklearn.metrics.accuracy_score(condlabels,pred)\n",
    "    return pred,confmtx,acc\n",
    "\n",
    "pred,confmtx,acc=run_classifier(fmri_masked,condlabels,runlabels,SVC(kernel='linear'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(confmtx)\n",
    "print('Accuracy score: %f'%acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the classifier repeatedly using random labels to get a null distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nperms=500\n",
    "randacc=numpy.zeros(nperms)\n",
    "condlabels_rand=condlabels.copy()\n",
    "for i in range(nperms):\n",
    "    _,_,randacc[i]=run_classifier(fmri_masked,condlabels_rand,\n",
    "                                  runlabels,\n",
    "                                  SVC(kernel='linear'),\n",
    "                                  shuffle_labels=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pct=scipy.stats.percentileofscore(randacc,acc)\n",
    "print('Pval:',(100-pct)/100.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set up a searchlight analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nilearn.decoding \n",
    "slradius=8\n",
    "sl=nilearn.decoding.SearchLight(mask_img=vtmaskimg,radius=slradius)\n",
    "sl.fit(nibabel.load(os.path.join(modeldir,'zstatdata_facecat.nii.gz')),condlabels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(haxbydata.boldfile.replace('.nii.gz','_brain_mean.nii.gz')):\n",
    "    mean=fsl.maths.MeanImage(in_file=haxbydata.boldfile,\n",
    "                             out_file=haxbydata.boldfile.replace('.nii.gz','_brain_mean.nii.gz'))\n",
    "    mean.run()\n",
    "\n",
    "mean_fmri=nibabel.load(haxbydata.boldfile.replace('.nii.gz','_brain_mean.nii.gz'))\n",
    "nilearn.plotting.plot_stat_map(nibabel.Nifti1Image(sl.scores_,\n",
    "                mean_fmri.get_affine()), mean_fmri,\n",
    "                title=\"Searchlight\", display_mode=\"z\", cut_coords=[-28,-24,-20,-16,-12],\n",
    "                colorbar=False,threshold=0.75)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Change the searchlight radius and see how it affects the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Obtain a null distribution by running the searchligh 500 times, so we can compare the observed results to those expected by chance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nruns=500\n",
    "maxscores=numpy.zeros(nruns)\n",
    "sl_rand=nilearn.decoding.SearchLight(mask_img=vtmaskimg,radius=slradius)\n",
    "\n",
    "for i in range(500):\n",
    "    cl=shuffle_within_runs(condlabels,runlabels)\n",
    "    sl_rand.fit(nibabel.load(os.path.join(modeldir,'zstatdata_facecat.nii.gz')),cl)\n",
    "    maxscores[i]=numpy.max(sl_rand.scores_)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cutoff=scipy.stats.scoreatpercentile(maxscores,95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('95% null accuracy: %f'%cutoff)\n",
    "print('N voxels > cutoff: %d'%numpy.sum(sl.scores_>cutoff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
